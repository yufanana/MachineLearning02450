{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e78eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import draw_neural_net\n",
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f613a",
   "metadata": {},
   "source": [
    "### One-hot-encoding for categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb30417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>weathersit_1</th>\n",
       "      <th>weathersit_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>1</td>\n",
       "      <td>0.254167</td>\n",
       "      <td>0.226642</td>\n",
       "      <td>0.652917</td>\n",
       "      <td>0.350133</td>\n",
       "      <td>2114</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>1</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.255046</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.155471</td>\n",
       "      <td>3095</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.752917</td>\n",
       "      <td>0.124383</td>\n",
       "      <td>1341</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0</td>\n",
       "      <td>0.255833</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.350754</td>\n",
       "      <td>1796</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1</td>\n",
       "      <td>0.215833</td>\n",
       "      <td>0.223487</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.154846</td>\n",
       "      <td>2729</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     workingday      temp     atemp       hum  windspeed   cnt  season_1  \\\n",
       "0             0  0.344167  0.363625  0.805833   0.160446   985         1   \n",
       "1             0  0.363478  0.353739  0.696087   0.248539   801         1   \n",
       "2             1  0.196364  0.189405  0.437273   0.248309  1349         1   \n",
       "3             1  0.200000  0.212122  0.590435   0.160296  1562         1   \n",
       "4             1  0.226957  0.229270  0.436957   0.186900  1600         1   \n",
       "..          ...       ...       ...       ...        ...   ...       ...   \n",
       "726           1  0.254167  0.226642  0.652917   0.350133  2114         1   \n",
       "727           1  0.253333  0.255046  0.590000   0.155471  3095         1   \n",
       "728           0  0.253333  0.242400  0.752917   0.124383  1341         1   \n",
       "729           0  0.255833  0.231700  0.483333   0.350754  1796         1   \n",
       "730           1  0.215833  0.223487  0.577500   0.154846  2729         1   \n",
       "\n",
       "     season_2  season_3  weathersit_1  weathersit_2  \n",
       "0           0         0             0             1  \n",
       "1           0         0             0             1  \n",
       "2           0         0             1             0  \n",
       "3           0         0             1             0  \n",
       "4           0         0             1             0  \n",
       "..        ...       ...           ...           ...  \n",
       "726         0         0             0             1  \n",
       "727         0         0             0             1  \n",
       "728         0         0             0             1  \n",
       "729         0         0             1             0  \n",
       "730         0         0             0             1  \n",
       "\n",
       "[731 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read file and store as pandas dataframe\n",
    "filename = '../Data/day.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "for attribute in ['instant','dteday','yr','mnth','holiday','weekday','casual','registered']:\n",
    "    df = df.drop(attribute, axis=1)\n",
    "    \n",
    "# One hot encoding\n",
    "ohe_df = pd.get_dummies(df, columns = ['season','weathersit'])\n",
    "ohe_df = ohe_df.drop(['season_4','weathersit_3'], axis=1)    # season_4, weathersit_3 are chosen as reference variables\n",
    "display(ohe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d36af",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cbb7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>weathersit_1</th>\n",
       "      <th>weathersit_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.826662</td>\n",
       "      <td>-0.679946</td>\n",
       "      <td>1.250171</td>\n",
       "      <td>-0.387892</td>\n",
       "      <td>-1.817953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.721095</td>\n",
       "      <td>-0.740652</td>\n",
       "      <td>0.479113</td>\n",
       "      <td>0.749602</td>\n",
       "      <td>-1.912999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.634657</td>\n",
       "      <td>-1.749767</td>\n",
       "      <td>-1.339274</td>\n",
       "      <td>0.746632</td>\n",
       "      <td>-1.629925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.614780</td>\n",
       "      <td>-1.610270</td>\n",
       "      <td>-0.263182</td>\n",
       "      <td>-0.389829</td>\n",
       "      <td>-1.519898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.467414</td>\n",
       "      <td>-1.504971</td>\n",
       "      <td>-1.341494</td>\n",
       "      <td>-0.046307</td>\n",
       "      <td>-1.500269</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.318665</td>\n",
       "      <td>-1.521108</td>\n",
       "      <td>0.175807</td>\n",
       "      <td>2.061426</td>\n",
       "      <td>-1.234757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.323224</td>\n",
       "      <td>-1.346690</td>\n",
       "      <td>-0.266238</td>\n",
       "      <td>-0.452131</td>\n",
       "      <td>-0.728012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.323224</td>\n",
       "      <td>-1.424344</td>\n",
       "      <td>0.878392</td>\n",
       "      <td>-0.853552</td>\n",
       "      <td>-1.634057</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.309558</td>\n",
       "      <td>-1.490049</td>\n",
       "      <td>-1.015664</td>\n",
       "      <td>2.069444</td>\n",
       "      <td>-1.399023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.528225</td>\n",
       "      <td>-1.540482</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.460201</td>\n",
       "      <td>-0.917073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     workingday      temp     atemp       hum  windspeed       cnt  season_1  \\\n",
       "0           0.0 -0.826662 -0.679946  1.250171  -0.387892 -1.817953       1.0   \n",
       "1           0.0 -0.721095 -0.740652  0.479113   0.749602 -1.912999       1.0   \n",
       "2           1.0 -1.634657 -1.749767 -1.339274   0.746632 -1.629925       1.0   \n",
       "3           1.0 -1.614780 -1.610270 -0.263182  -0.389829 -1.519898       1.0   \n",
       "4           1.0 -1.467414 -1.504971 -1.341494  -0.046307 -1.500269       1.0   \n",
       "..          ...       ...       ...       ...        ...       ...       ...   \n",
       "726         1.0 -1.318665 -1.521108  0.175807   2.061426 -1.234757       1.0   \n",
       "727         1.0 -1.323224 -1.346690 -0.266238  -0.452131 -0.728012       1.0   \n",
       "728         0.0 -1.323224 -1.424344  0.878392  -0.853552 -1.634057       1.0   \n",
       "729         0.0 -1.309558 -1.490049 -1.015664   2.069444 -1.399023       1.0   \n",
       "730         1.0 -1.528225 -1.540482 -0.354061  -0.460201 -0.917073       1.0   \n",
       "\n",
       "     season_2  season_3  weathersit_1  weathersit_2  \n",
       "0         0.0       0.0           0.0           1.0  \n",
       "1         0.0       0.0           0.0           1.0  \n",
       "2         0.0       0.0           1.0           0.0  \n",
       "3         0.0       0.0           1.0           0.0  \n",
       "4         0.0       0.0           1.0           0.0  \n",
       "..        ...       ...           ...           ...  \n",
       "726       0.0       0.0           0.0           1.0  \n",
       "727       0.0       0.0           0.0           1.0  \n",
       "728       0.0       0.0           0.0           1.0  \n",
       "729       0.0       0.0           1.0           0.0  \n",
       "730       0.0       0.0           0.0           1.0  \n",
       "\n",
       "[731 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N, M = ohe_df.shape\n",
    "attribute_names = list(ohe_df.columns)\n",
    "\n",
    "# Get column indexes\n",
    "temp_col = ohe_df.columns.get_loc(\"temp\")\n",
    "atemp_col = ohe_df.columns.get_loc(\"atemp\")\n",
    "hum_col = ohe_df.columns.get_loc(\"hum\")\n",
    "wspd_col = ohe_df.columns.get_loc(\"windspeed\")\n",
    "cnt_col = ohe_df.columns.get_loc(\"cnt\")\n",
    "\n",
    "# Undo the original max-min normalization\n",
    "data = ohe_df.values\n",
    "for row in range(0, N):\n",
    "    data[row, temp_col] = data[row, temp_col]*(39-(-8)) + (-8)\n",
    "    data[row, atemp_col] = data[row, atemp_col]*(50-(-16)) + (-16)\n",
    "    data[row, hum_col] = data[row, hum_col]*100\n",
    "    data[row, wspd_col] = data[row, wspd_col]*67\n",
    "\n",
    "# Standarize ratio data attributes\n",
    "for col in range(temp_col, cnt_col+1): # subtract mean column-wise\n",
    "    mn = data[:, col].mean(0)\n",
    "    std = np.std(data[:, col])\n",
    "    data[:, col] = (data[:, col] - np.ones(N)*mn)/std\n",
    "\n",
    "# Create DataFrame for visualisation\n",
    "data_df = pd.DataFrame(data, columns=attribute_names)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ab082",
   "metadata": {},
   "source": [
    "### Set 'cnt' as target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7066153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 731, M: 11 (including offset)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into features and target vector\n",
    "cnt_col = attribute_names.index(\"cnt\")\n",
    "y = data[:,cnt_col]\n",
    "X = np.delete(data, cnt_col, axis=1)\n",
    "attribute_names.pop(cnt_col)\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "X = np.concatenate((np.ones((X.shape[0],1)),X),1)\n",
    "attribute_names = [u'offset']+attribute_names\n",
    "M = M+1\n",
    "\n",
    "print(\"N: {}, M: {} (including offset)\".format(N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74c263b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['offset',\n",
       " 'workingday',\n",
       " 'temp',\n",
       " 'atemp',\n",
       " 'hum',\n",
       " 'windspeed',\n",
       " 'season_1',\n",
       " 'season_2',\n",
       " 'season_3',\n",
       " 'weathersit_1',\n",
       " 'weathersit_2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd851d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_net(model, h, loss_fn, X, y,\n",
    "                     n_replicates=3, max_iter=10000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Train a neural network with PyTorch based on a training set consisting of\n",
    "    observations X and class y. The model and loss_fn inputs define the\n",
    "    architecture to train and the cost-function update the weights based on,\n",
    "    respectively.\n",
    "        \n",
    "    Args:\n",
    "        model:          A function handle to make a torch.nn.Sequential.\n",
    "        loss_fn:        A torch.nn-loss, e.g.  torch.nn.BCELoss() for binary \n",
    "                        binary classification, torch.nn.CrossEntropyLoss() for\n",
    "                        multiclass classification, or torch.nn.MSELoss() for\n",
    "                        regression (see https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "        n_replicates:   An integer specifying number of replicates to train,\n",
    "                        the neural network with the lowest loss is returned.\n",
    "        max_iter:       An integer specifying the maximum number of iterations\n",
    "                        to do (default 10000).\n",
    "        tolerenace:     A float describing the tolerance/convergence criterion\n",
    "                        for minimum relative change in loss (default 1e-6)\n",
    "                        \n",
    "        \n",
    "    Returns:\n",
    "        A list of three elements:\n",
    "            best_net:       A trained torch.nn.Sequential that had the lowest \n",
    "                            loss of the trained replicates\n",
    "            final_loss:     An float specifying the loss of best performing net\n",
    "            learning_curve: A list containing the learning curve of the best net.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import torch\n",
    "    # Specify maximum number of iterations for training\n",
    "    logging_frequency = 5000 # display the loss every 1000th iteration\n",
    "    best_final_loss = 1e100\n",
    "    print('\\n\\tTraining ANN with h={} hidden units'.format(h))\n",
    "    \n",
    "    for r in range(n_replicates):\n",
    "        print('\\n\\tReplicate: {}/{}'.format(r+1, n_replicates))\n",
    "        # Make a new net (calling model() makes a new initialization of weights) \n",
    "        net = model(h)\n",
    "        \n",
    "        # initialize weights based on limits that scale with number of in- and\n",
    "        # outputs to the layer, increasing the chance that we converge to \n",
    "        # a good solution\n",
    "        torch.nn.init.xavier_uniform_(net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(net[2].weight)\n",
    "                     \n",
    "        # We can optimize the weights by means of stochastic gradient descent\n",
    "        # The learning rate, lr, can be adjusted if training doesn't perform as\n",
    "        # intended try reducing the lr. If the learning curve hasn't converged\n",
    "        # (i.e. \"flattend out\"), you can try increasing the maximum number of\n",
    "        # iterations, but also potentially increasing the learning rate:\n",
    "        # optimizer = torch.optim.SGD(net.parameters(), lr = 5e-3)\n",
    "        \n",
    "        # A more complicated optimizer is the Adam-algortihm, which is an extension\n",
    "        # of SGD to adaptively change the learing rate, which is widely used:\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "        \n",
    "        # Train the network while displaying and storing the loss\n",
    "        print('\\t\\t{}\\t{}\\t\\t\\t{}'.format('Iter', 'Loss','Rel. loss'))\n",
    "        learning_curve = [] # setup storage for loss at each step\n",
    "        old_loss = 1e6\n",
    "        for i in range(max_iter):\n",
    "            y_est = net(X) # forward pass, predict labels on training set\n",
    "            loss = loss_fn(y_est, y) # determine loss\n",
    "            loss_value = loss.data.numpy() #get numpy array instead of tensor\n",
    "            learning_curve.append(loss_value) # record loss for later display\n",
    "            \n",
    "            # Convergence check, see if the percentual loss decrease is within\n",
    "            # tolerance:\n",
    "            p_delta_loss = np.abs(loss_value-old_loss)/old_loss\n",
    "            if p_delta_loss < tolerance: break\n",
    "            old_loss = loss_value\n",
    "            \n",
    "            # display loss with some frequency:\n",
    "            if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                print(print_str)\n",
    "            # do backpropagation of loss and optimize weights \n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            \n",
    "            \n",
    "        # display final loss\n",
    "        print('\\t\\tFinal loss:')\n",
    "        print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "        print(print_str)\n",
    "        \n",
    "        # Comparing final loss values among replicates\n",
    "        if loss_value < best_final_loss: \n",
    "            best_net = net\n",
    "            best_final_loss = loss_value\n",
    "            best_learning_curve = learning_curve\n",
    "        \n",
    "    # Return the best curve along with its final loss and learing curve\n",
    "    return best_net, best_final_loss, best_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75170046",
   "metadata": {},
   "source": [
    "### 1. Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b6d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of lambda\n",
    "lambdas = np.power(10.,np.linspace(-5,9,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f2d57",
   "metadata": {},
   "source": [
    "### 2. Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80d9cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model of type:\n",
      "\n",
      "Sequential(\n",
      "  (0): Linear(in_features=11, out_features=2, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the ANN model structure\n",
    "h_values = [2,4,6]\n",
    "n_replicates = 1        # number of networks trained in each k-fold\n",
    "max_iter = 20000\n",
    "\n",
    "model = lambda n_hidden_units: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "                    # 1st transfer function, either Tanh or ReLU:\n",
    "                    torch.nn.Tanh(),                            #torch.nn.ReLU()   \n",
    "                    torch.nn.Linear(n_hidden_units, 1) # H hidden units to 1 output neuron\n",
    "                    )\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "print('Training model of type:\\n\\n{}'.format(str(model(h_values[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89e582",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba99068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "K_outer = 10\n",
    "K_inner = 10\n",
    "\n",
    "cv_outer = model_selection.KFold(K_outer, shuffle=False)\n",
    "cv_inner = model_selection.KFold(K_inner, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d1f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "Error_par_lr = np.empty((K_outer,1))\n",
    "Error_test_lr = np.empty((K_outer,1))\n",
    "Error_par_rlr = np.empty((K_outer,1))\n",
    "Error_test_rlr = np.empty((K_outer,1))\n",
    "Error_par_nofeatures = np.empty((K_outer,1))\n",
    "Error_test_nofeatures = np.empty((K_outer,1))\n",
    "\n",
    "Error_test_ANN = np.zeros((K_outer,1))\n",
    "\n",
    "opt_h_values = np.zeros((K_outer,1))            # optimal ANN hidden units for each outer fold\n",
    "opt_lambdas = np.empty((K_outer,1))             # optimal lambdas for each outer fold\n",
    "w_rlr = np.empty((M,K_outer))                   # weights for each attribute with regularisation\n",
    "w_noreg = np.empty((M,K_outer))                 # weights for each attribute without regularisation\n",
    "mu = np.empty((K_outer, M-1))\n",
    "sigma = np.empty((K_outer, M-1))\n",
    "\n",
    "# Statistical evaluation\n",
    "y_hat = np.zeros((1,3))       # estimates for each model [ANN, rlr, baseline], to delete 1st row at the end\n",
    "y_true = np.zeros((1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a66ba",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103a176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer Cross Validation Fold: 1/10\n",
      "\n",
      "\tInner Fold: 1/10\n",
      "\n",
      "\tTraining ANN with h=2 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.38797837\t3.6102633e-06\n",
      "\t\t10000\t0.3451989\t5.352665e-06\n",
      "\t\tFinal loss:\n",
      "\t\t11333\t0.34399125\t9.530046e-07\n",
      "\n",
      "\tBest loss: 0.3439912497997284\n",
      "\n",
      "\n",
      "\tTraining ANN with h=4 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.3194744\t3.6381205e-06\n",
      "\t\tFinal loss:\n",
      "\t\t6388\t0.31839347\t8.424189e-07\n",
      "\n",
      "\tBest loss: 0.3183934688568115\n",
      "\n",
      "\n",
      "\tTraining ANN with h=6 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.28923097\t1.6486109e-05\n",
      "\t\tFinal loss:\n",
      "\t\t8807\t0.27173817\t0.0\n",
      "\n",
      "\tBest loss: 0.2717381715774536\n",
      "\n",
      "\n",
      "\tInner Fold: 2/10\n",
      "\n",
      "\tTraining ANN with h=2 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.3433472\t4.513547e-06\n",
      "\t\tFinal loss:\n",
      "\t\t9152\t0.3378744\t9.702576e-07\n",
      "\n",
      "\tBest loss: 0.3378744125366211\n",
      "\n",
      "\n",
      "\tTraining ANN with h=4 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.28565583\t1.1267456e-05\n",
      "\t\tFinal loss:\n",
      "\t\t9605\t0.27648225\t9.701188e-07\n",
      "\n",
      "\tBest loss: 0.2764822542667389\n",
      "\n",
      "\n",
      "\tTraining ANN with h=6 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.25879386\t8.0610325e-06\n",
      "\t\t10000\t0.24652822\t5.439934e-06\n",
      "\t\tFinal loss:\n",
      "\t\t12080\t0.24471031\t9.742881e-07\n",
      "\n",
      "\tBest loss: 0.244710311293602\n",
      "\n",
      "\n",
      "\tInner Fold: 3/10\n",
      "\n",
      "\tTraining ANN with h=2 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.3492431\t1.0751973e-05\n",
      "\t\tFinal loss:\n",
      "\t\t8287\t0.33764154\t8.826617e-08\n",
      "\n",
      "\tBest loss: 0.33764153718948364\n",
      "\n",
      "\n",
      "\tTraining ANN with h=4 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.30283776\t1.2989976e-05\n",
      "\t\t10000\t0.29366294\t9.336514e-06\n",
      "\t\tFinal loss:\n",
      "\t\t11058\t0.28817913\t7.239112e-07\n",
      "\n",
      "\tBest loss: 0.2881791293621063\n",
      "\n",
      "\n",
      "\tTraining ANN with h=6 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.2762074\t4.0568135e-05\n",
      "\t\tFinal loss:\n",
      "\t\t8627\t0.25925392\t6.8972463e-07\n",
      "\n",
      "\tBest loss: 0.25925391912460327\n",
      "\n",
      "\n",
      "\tInner Fold: 4/10\n",
      "\n",
      "\tTraining ANN with h=2 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.34330505\t1.28477195e-05\n",
      "\t\t10000\t0.3329524\t9.040353e-06\n",
      "\t\t15000\t0.32273605\t3.0473002e-06\n",
      "\t\tFinal loss:\n",
      "\t\t18224\t0.32063767\t9.2946954e-07\n",
      "\n",
      "\tBest loss: 0.32063767313957214\n",
      "\n",
      "\n",
      "\tTraining ANN with h=4 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.28689292\t1.3088658e-05\n",
      "\t\t10000\t0.27240553\t4.8137636e-06\n",
      "\t\tFinal loss:\n",
      "\t\t12895\t0.26864904\t9.984053e-07\n",
      "\n",
      "\tBest loss: 0.26864904165267944\n",
      "\n",
      "\n",
      "\tTraining ANN with h=6 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.256098\t1.477887e-05\n",
      "\t\t10000\t0.23224486\t1.6104259e-05\n",
      "\t\tFinal loss:\n",
      "\t\t10139\t0.2318192\t7.7135024e-07\n",
      "\n",
      "\tBest loss: 0.23181919753551483\n",
      "\n",
      "\n",
      "\tInner Fold: 5/10\n",
      "\n",
      "\tTraining ANN with h=2 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.3612862\t1.1713374e-05\n",
      "\t\t10000\t0.3264494\t1.0955066e-06\n",
      "\t\tFinal loss:\n",
      "\t\t10042\t0.32643384\t9.1296585e-07\n",
      "\n",
      "\tBest loss: 0.32643383741378784\n",
      "\n",
      "\n",
      "\tTraining ANN with h=4 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.3154044\t7.653571e-06\n",
      "\t\t10000\t0.29587904\t1.3295482e-05\n",
      "\t\tFinal loss:\n",
      "\t\t10444\t0.2944828\t7.084163e-07\n",
      "\n",
      "\tBest loss: 0.2944827973842621\n",
      "\n",
      "\n",
      "\tTraining ANN with h=6 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.29354388\t1.1370777e-05\n",
      "\t\t10000\t0.275417\t4.977543e-06\n",
      "\t\tFinal loss:\n",
      "\t\t11622\t0.27320972\t9.81739e-07\n",
      "\n",
      "\tBest loss: 0.27320972084999084\n",
      "\n",
      "\n",
      "\tInner Fold: 6/10\n",
      "\n",
      "\tTraining ANN with h=2 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.379626\t5.6522867e-06\n",
      "\t\tFinal loss:\n",
      "\t\t9329\t0.37432024\t9.554053e-07\n",
      "\n",
      "\tBest loss: 0.37432023882865906\n",
      "\n",
      "\n",
      "\tTraining ANN with h=4 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t5000\t0.32178244\t4.7234125e-06\n",
      "\t\tFinal loss:\n",
      "\t\t8262\t0.3163268\t2.8264105e-07\n",
      "\n",
      "\tBest loss: 0.3163267970085144\n",
      "\n",
      "\n",
      "\tTraining ANN with h=6 hidden units\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    }
   ],
   "source": [
    "# Setup figure for display of ANN summaries\n",
    "summaries, summaries_axes = plt.subplots(1, 2, figsize=(10,5),num=2)\n",
    "color_list = ['tab:orange', 'tab:green', 'tab:purple', 'tab:brown', 'tab:pink',\n",
    "              'tab:gray', 'tab:olive', 'tab:cyan', 'tab:red', 'tab:blue']\n",
    "\n",
    "for k_out, (par_index, test_index) in enumerate(cv_outer.split(X,y)):\n",
    "    \n",
    "    print('\\nOuter Cross Validation Fold: {0}/{1}'.format(k_out+1,K_outer))\n",
    "    \n",
    "    # Split outer fold into parameterisation set and test set\n",
    "    X_par = X[par_index]\n",
    "    y_par = y[par_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    y_par = y_par.squeeze()\n",
    "    \n",
    "    # Initialise error arrays\n",
    "    w = np.empty((M,K_inner,len(lambdas)))           \n",
    "    train_error = np.empty((K_inner,len(lambdas)))\n",
    "    val_error = np.empty((K_inner,len(lambdas)))    \n",
    "    test_error = np.empty((K_inner,len(lambdas)))   \n",
    "    \n",
    "    ann_val_error = np.empty((K_inner,len(h_values)))  \n",
    "    ann_test_error = np.empty((K_inner,len(h_values)))\n",
    "    \n",
    "    # TODO: rename val_error, test_error, lambdas\n",
    "    \n",
    "    for k_in, (train_index, val_index) in enumerate(cv_inner.split(X_par,y_par)):\n",
    "    \n",
    "        print('\\n\\tInner Fold: {}/{}'.format(k_in+1,K_inner))\n",
    "        \n",
    "        # Split parameterisation set into training set and validation set\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_val = X[val_index]\n",
    "        y_val = y[val_index]\n",
    "    \n",
    "        ########################### Lingear Regression ###########################\n",
    "        # Standardize inner fold based on training set, and save the mean and std\n",
    "        mu[k_in, :] = np.mean(X_train[:, 1:], 0)\n",
    "        sigma[k_in, :] = np.std(X_train[:, 1:], 0)\n",
    "        X_train[:, 1:] = (X_train[:, 1:] - mu[k_in, :] ) / sigma[k_in, :]\n",
    "        X_val[:, 1:] = (X_val[:, 1:] - mu[k_in, :] ) / sigma[k_in, :]\n",
    "\n",
    "        # Precompute terms\n",
    "        Xty = X_train.T @ y_train\n",
    "        XtX = X_train.T @ X_train\n",
    "\n",
    "        # solve for weights\n",
    "        for l in range(0,len(lambdas)):\n",
    "            # Compute parameters for current value of lambda and current CV fold\n",
    "            lambdaI = lambdas[l] * np.eye(M)\n",
    "            lambdaI[0,0] = 0 # remove bias regularization\n",
    "            w[:,k_in,l] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "            \n",
    "            # Evaluate training and validation performance\n",
    "            train_error[k_in,l] = np.power(y_train-X_train @ w[:,k_in,l].T,2).mean(axis=0)\n",
    "            val_error[k_in,l] = np.power(y_val-X_val @ w[:,k_in,l].T,2).mean(axis=0)\n",
    "    \n",
    "        ################################## ANN ##################################\n",
    "        # Convert train and validation sets into tensors\n",
    "        X_train = torch.Tensor(X[train_index,:])\n",
    "        y_train = torch.Tensor(y[train_index]).reshape(-1,1)\n",
    "        X_val = torch.Tensor(X[val_index,:])\n",
    "        y_val = torch.Tensor(y[val_index]).reshape(-1,1)\n",
    "\n",
    "        # Train the ANN using training dataset\n",
    "        for h_enum, h in enumerate(h_values):\n",
    "            net, best_final_loss, learning_curve = train_neural_net(model, \n",
    "                                                                   h,\n",
    "                                                                   loss_fn,\n",
    "                                                                   X=X_train,\n",
    "                                                                   y=y_train,\n",
    "                                                                   n_replicates=n_replicates,\n",
    "                                                                   max_iter=max_iter)\n",
    "            print('\\n\\tBest loss: {}\\n'.format(best_final_loss))\n",
    "\n",
    "            # Determine estimated values for validation set\n",
    "            y_val_est = net(X_val)\n",
    "            \n",
    "            # Evaluate validation performance\n",
    "            se_val = (y_val_est.squeeze().float()-y_val.squeeze().float())**2  # squared error\n",
    "            mse_val = (sum(se_val).type(torch.float)/len(y_val)).data.numpy()      # mean squared error\n",
    "            ann_val_error[k_in,h_enum] = mse_val\n",
    "\n",
    "\n",
    "    ########################### Lingear Regression ###########################\n",
    "    opt_val_err = np.min(np.mean(val_error,axis=0))        # get mean val_error for each lambda value, then find min\n",
    "    opt_lambda = lambdas[np.argmin(np.mean(val_error,axis=0))]\n",
    "    print(\"Optimal lambda found: \", round(np.log10(opt_lambda),5))   # TODO: check lambda value found at each outer fold\n",
    "    train_err_vs_lambda = np.mean(train_error,axis=0)\n",
    "    val_err_vs_lambda = np.mean(val_error,axis=0)\n",
    "    mean_w_vs_lambda = np.squeeze(np.mean(w,axis=1))\n",
    "    opt_lambdas[k_out] = opt_lambda              # Save optimal RLR hyperparameter from inner fold\n",
    "\n",
    "    # Retrain linear regression model on parameterisation set using optimal lambda\n",
    "    Xty_par = X_par.T @ y_par\n",
    "    XtX_par = X_par.T @ X_par\n",
    "    \n",
    "    #### Estimate weights using parameterisation set\n",
    "    # for the optimal value of lambda\n",
    "    opt_lambda_I = opt_lambda * np.eye(M)\n",
    "    opt_lambda_I[0,0] = 0 # remove bias regularization\n",
    "    w_rlr[:,k_out] = np.linalg.solve(XtX_par+opt_lambda_I,Xty_par).squeeze()\n",
    "    \n",
    "    # for unregularized linear regression\n",
    "    w_noreg[:,k_out] = np.linalg.solve(XtX_par,Xty_par).squeeze()\n",
    "    \n",
    "    #### Determine RLR test error \n",
    "    # MSE with regularization with optimal lambda\n",
    "    Error_par_rlr[k_out] = np.square(y_par-X_par @ w_rlr[:,k_out]).sum(axis=0)/y_par.shape[0]\n",
    "    Error_test_rlr[k_out] = np.square(y_test-X_test @ w_rlr[:,k_out]).sum(axis=0)/y_test.shape[0]\n",
    "    \n",
    "    # MSE without using the input data\n",
    "    Error_par_nofeatures[k_out] = np.square(y_par-y_par.mean()).sum(axis=0)/y_par.shape[0]\n",
    "    Error_test_nofeatures[k_out] = np.square(y_test-y_test.mean()).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    # MSE without regularization\n",
    "    Error_par_lr[k_out] = np.square(y_par-X_par @ w_noreg[:,k_out]).sum(axis=0)/y_par.shape[0]\n",
    "    Error_test_lr[k_out] = np.square(y_test-X_test @ w_noreg[:,k_out]).sum(axis=0)/y_test.shape[0]\n",
    "    \n",
    "    #### For statistical evaluation\n",
    "    y_hat_rlr = X_test @ w_rlr[:,k_out]\n",
    "    y_hat_rlr = np.expand_dims(y_hat_rlr, axis=1)\n",
    "#     print(\"y_hat_rlr: \", y_hat_rlr.shape)\n",
    "    y_hat_base = np.full((y_test.shape[0], 1), y_test.mean())\n",
    "    \n",
    "#     print(\"y_test: \", y_test.shape)\n",
    "#     print(\"y_true before: \", y_true.shape)\n",
    "    y_true = np.append(y_true, y_test, axis=0)\n",
    "#     print(\"y_true after: \", y_true.shape)\n",
    "#     print(\"y_hat_base: \", y_hat_base.shape)\n",
    "    \n",
    "    ##################################### ANN #####################################\n",
    "    # Convert test set into tensors\n",
    "    X_par = torch.Tensor(X[par_index])\n",
    "    y_par = torch.Tensor(y[par_index]).reshape(-1,1)\n",
    "    X_test = torch.Tensor(X[test_index])\n",
    "    y_test = torch.Tensor(y[test_index]).reshape(-1,1)\n",
    "    \n",
    "    opt_ann_val_err = np.min(np.mean(ann_val_error,axis=0))\n",
    "    opt_h = h_values[np.argmin(np.mean(ann_val_error,axis=0))]\n",
    "    opt_h_values[k_out] = opt_h              # Save optimal ANN hyperparameter from inner fold\n",
    "    \n",
    "    # Retrain ANN on parameterisation set using optimal number of hidden units\n",
    "    print(\"\\tRetraining ANN using optimal number of hidden units\")\n",
    "    opt_net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                           opt_h,\n",
    "                                                           loss_fn,\n",
    "                                                           X=X_par,\n",
    "                                                           y=y_par,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "    y_test_est = opt_net(X_test)\n",
    "    \n",
    "    # Determine ANN test error\n",
    "    se_test = (y_test_est.squeeze().float()-y_test.squeeze().float())**2     # squared error\n",
    "    mse_test = (sum(se_test).type(torch.float)/len(y_test)).data.numpy()\n",
    "    Error_test_ANN[k_out] = mse_test                     # store error rate for current CV fold\n",
    "    \n",
    "    print(\"\\n\\tTest Error for Outer Fold {}/{}\".format(k_out+1,K_outer))\n",
    "    print(\"\\n\\t\\tRLR: {}\".format(np.round(mse_test,3)))\n",
    "    print(\"\\n\\t\\tANN: {:.5e}\".format(Error_test_rlr[k_out][0]))\n",
    "    \n",
    "    # For statistical evaluation\n",
    "    y_hat_ann = y_test_est.float()\n",
    "    y_hat_ann = y_hat_ann.detach().numpy()\n",
    "#     print(\"y_hat_ann: \", y_hat_ann.shape)\n",
    "    \n",
    "    ############################### Statistical Evaluation ###############################\n",
    "    y_hat_fold = np.concatenate((y_hat_ann, y_hat_rlr, y_hat_base), axis=1)\n",
    "    y_hat = np.append(y_hat, y_hat_fold, axis=0)\n",
    "#     print(\"y_hat.shape: \", y_hat.shape)\n",
    "    \n",
    "    ############################### DISPLAY RESULTS ###############################\n",
    "    # Display the learning curve for the best net in the current outer fold\n",
    "    h, = summaries_axes[0].plot(learning_curve, color=color_list[k_out])\n",
    "    h.set_label('CV fold {0}'.format(k_out+1))\n",
    "    summaries_axes[0].set_xlabel('Iterations')\n",
    "    summaries_axes[0].set_ylabel('Loss')\n",
    "    summaries_axes[0].set_title('Learning curves')\n",
    "\n",
    "   # Display the results for the last outer cross-validation fold\n",
    "    if k_out == K_outer-1:\n",
    "        reg_fig, reg_axes = plt.subplots(1, 2, figsize=(10,5),num=1)\n",
    "        reg_axes[0].semilogx(lambdas,mean_w_vs_lambda.T[:,1:],'-',label=attribute_names[1:]) # Don't plot the bias term\n",
    "        reg_axes[0].set_xlabel('Regularization factor')\n",
    "        reg_axes[0].set_ylabel('Mean Coefficient Values')\n",
    "        reg_axes[0].grid()\n",
    "        reg_axes[0].legend()\n",
    "\n",
    "        reg_axes[1].set_title('Optimal lambda: 1e{0}'.format(round(np.log10(opt_lambdas[k_out][0]),5)))\n",
    "        reg_axes[1].loglog(lambdas,train_err_vs_lambda.T,'b-',lambdas,val_err_vs_lambda.T,'r-')\n",
    "        reg_axes[1].set_xlabel('Regularization factor')\n",
    "        reg_axes[1].set_ylabel('Squared error (crossvalidation)')\n",
    "        reg_axes[1].legend(['Train error','Validation error'])\n",
    "        reg_axes[1].grid()\n",
    "        reg_fig.tight_layout()\n",
    "\n",
    "y_hat = np.delete(y_hat, 0, 0)      # delete the 1st row of zeros\n",
    "y_true = np.delete(y_true, 0, 0)      # delete the 1st row of zeros\n",
    "        \n",
    "# Display the MSE across folds\n",
    "summaries_axes[1].bar(np.arange(1, K_outer+1), np.squeeze(np.asarray(Error_test_ANN)), color=color_list)\n",
    "summaries_axes[1].set_xlabel('Fold')\n",
    "summaries_axes[1].set_xticks(np.arange(1, K_outer+1))\n",
    "summaries_axes[1].set_ylabel('MSE')\n",
    "summaries_axes[1].set_title('Test mean-squared-error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103de95",
   "metadata": {},
   "source": [
    "### Results of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f73aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline without using the input data:')\n",
    "print('- Test error:     {0}\\n'.format(Error_test_nofeatures.mean()))\n",
    "\n",
    "print('Linear regression without feature selection:')\n",
    "print('- Training error: {0}'.format(Error_par_lr.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test_lr.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_par_nofeatures.sum()-Error_par_lr.sum())/Error_par_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test_lr.sum())/Error_test_nofeatures.sum()))\n",
    "\n",
    "print('Regularized linear regression:')\n",
    "print('- Training error: {0}'.format(Error_par_rlr.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test_rlr.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_par_nofeatures.sum()-Error_par_rlr.sum())/Error_par_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test_rlr.sum())/Error_test_nofeatures.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of RLR weights\n",
    "weights = range(1,M)      # skip offset\n",
    "bw = 1.0/(len(weights)+1)\n",
    "r = np.arange(1,K_outer+1)\n",
    "\n",
    "plt.figure(figsize=(6,6), num=3)\n",
    "for i in weights:\n",
    "    plt.bar(r+i*bw, w_rlr[i,:], width=bw)\n",
    "plt.xticks(r+bw, range(1,K_outer+1))\n",
    "plt.xlabel('Attributes')\n",
    "plt.ylabel('Weights')\n",
    "plt.legend(attribute_names[1:M+1], loc=(1.04, 0))\n",
    "plt.grid()\n",
    "plt.title('Weights from each fold of Regularized Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa5469",
   "metadata": {},
   "source": [
    "### Results of ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b13e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5),num=5)\n",
    "y_est = y_test_est.data.numpy().squeeze()\n",
    "y_true = y_test.data.numpy().squeeze()\n",
    "axis_range = [np.min([y_est, y_true])-1,np.max([y_est, y_true])+1]\n",
    "plt.plot(axis_range,axis_range,'k--')           # perfect estimation\n",
    "plt.plot(y_true, y_est,'ob',alpha=.25)          # ANN estimation\n",
    "plt.legend(['Perfect estimation','Model estimations'])\n",
    "plt.title('cnt: estimated VS true value (for last CV-fold)')\n",
    "plt.ylim(axis_range)\n",
    "plt.xlim(axis_range)\n",
    "plt.xlabel('True value')\n",
    "plt.ylabel('Estimated value')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a387ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Diagram of best neural net in last outer fold\n",
    "weights = [net[i].weight.data.numpy().T for i in [0,2]]\n",
    "biases = [net[i].bias.data.numpy() for i in [0,2]]\n",
    "tf =  [str(net[i]) for i in [1,2]]\n",
    "draw_neural_net(weights, biases, tf, attribute_names=attribute_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7016d51",
   "metadata": {},
   "source": [
    "### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c20db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Outer fold \\tANN \\t\\t\\tLinear Regression \\t\\tBaseline\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"i \\t\\th_i \\tEtest_i \\tlambda_i \\tEtest_i \\tEtest_i\")\n",
    "for i in range(K_outer):\n",
    "    print(\"{} \\t\\t{:.3f} \\t{:.3f} \\t\\t1e{} \\t{:.2f} \\t\\t{:.3f}\".format(\\\n",
    "        i+1,opt_h_values[i][0],Error_test_ANN[i].item(),round(np.log10(opt_lambdas[i][0]),4),\\\n",
    "            Error_test_rlr[i][0],Error_test_nofeatures[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3984005",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5cd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_error.shape\n",
    "opt_val_err = np.min(np.mean(val_error,axis=0))        # get mean val_error for each lambda value, then find min\n",
    "opt_lambda = lambdas[np.argmin(np.mean(val_error,axis=0))]\n",
    "opt_lambdas       # TODO: why are all lambdas the same? code error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53033645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_test_ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d69a96",
   "metadata": {},
   "source": [
    "## Saved Results Below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29181c0d",
   "metadata": {},
   "source": [
    "# Test values for K1=K2=5 with ANN and RLR code complete, using optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "Outer fold \tANN \t\t\tLinear Regression \t\tBaseline\n",
    "--------------------------------------------------------------------------------\n",
    "i \t\th_i \tEtest_i \tlambda_i \tEtest_i \tEtest_i\n",
    "1 \t\t2.000 \t1.203 \t\t1e2.3535 \t1.03 \t\t0.434\n",
    "2 \t\t2.000 \t1.280 \t\t1e2.3535 \t1.04 \t\t0.173\n",
    "3 \t\t2.000 \t0.642 \t\t1e2.3535 \t0.21 \t\t0.272\n",
    "4 \t\t4.000 \t1.231 \t\t1e2.3535 \t0.90 \t\t0.327\n",
    "5 \t\t2.000 \t1.174 \t\t1e2.3535 \t1.35 \t\t0.941"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41f43467",
   "metadata": {},
   "source": [
    "Test values for Reg Lin Regression only. ANN was not completed then.\n",
    "\n",
    "\n",
    "K_outer = 4, K_inner = 4\n",
    "# Outer fold \tANN \t\t\tLinear Regression \t\tBaseline\n",
    "--------------------------------------------------------------------------------\n",
    "i \t\thâˆ—_i \tEtest_i \tlambda_i \tEtest_i \tEtest_i\n",
    "1 \t\t0.000 \t0.581 \t\t1.63e+02 \t0.93 \t\t0.581\n",
    "2 \t\t0.000 \t0.307 \t\t4.33e+02 \t0.67 \t\t0.307\n",
    "3 \t\t0.000 \t0.773 \t\t1.00e-05 \t1.18 \t\t0.773\n",
    "4 \t\t0.000 \t0.817 \t\t1.00e-05 \t1.39 \t\t0.817\n",
    "\n",
    "\n",
    "K_outer = 5, K_inner = 5\n",
    "Outer fold \tANN \t\t\tLinear Regression \t\tBaseline\n",
    "--------------------------------------------------------------------------------\n",
    "i \t\thâˆ—_i \tEtest_i \tlambda_i \tEtest_i \tEtest_i\n",
    "1 \t\t0.000 \t0.000 \t\t1.18e+02 \t0.95 \t\t0.434\n",
    "2 \t\t0.000 \t0.000 \t\t3.05e+03 \t0.26 \t\t0.173\n",
    "3 \t\t0.000 \t0.000 \t\t6.14e+01 \t0.27 \t\t0.272\n",
    "4 \t\t0.000 \t0.000 \t\t1.00e-05 \t1.13 \t\t0.327\n",
    "5 \t\t0.000 \t0.000 \t\t1.00e-05 \t1.31 \t\t0.941\n",
    "\n",
    "\n",
    "K_outer = 10, K_inner = 10\n",
    "Outer fold \tANN \t\t\tLinear Regression \t\tBaseline\n",
    "--------------------------------------------------------------------------------\n",
    "i \t\thâˆ—_i \tEtest_i \tlambda_i \tEtest_i \tEtest_i\n",
    "1 \t\t0.000 \t0.000 \t\t6.14e+01 \t0.90 \t\t0.063\n",
    "2 \t\t0.000 \t0.000 \t\t1.63e+02 \t0.54 \t\t0.335\n",
    "3 \t\t0.000 \t0.000 \t\t2.98e+04 \t0.09 \t\t0.086\n",
    "4 \t\t0.000 \t0.000 \t\t1.12e+04 \t0.26 \t\t0.245\n",
    "5 \t\t0.000 \t0.000 \t\t8.50e+01 \t0.34 \t\t0.245\n",
    "6 \t\t0.000 \t0.000 \t\t1.67e+01 \t0.44 \t\t0.275\n",
    "7 \t\t0.000 \t0.000 \t\t1.00e-05 \t1.03 \t\t0.394\n",
    "8 \t\t0.000 \t0.000 \t\t1.00e-05 \t0.50 \t\t0.200\n",
    "9 \t\t0.000 \t0.000 \t\t1.00e-05 \t1.31 \t\t0.275\n",
    "10 \t\t0.000 \t0.000 \t\t1.00e-05 \t0.59 \t\t0.924"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course02450",
   "language": "python",
   "name": "course02450"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e78eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dill import dump_session, load_session\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection, metrics\n",
    "from toolbox_02450 import mcnemar\n",
    "import scipy.stats as st\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f613a",
   "metadata": {},
   "source": [
    "### One-hot-encoding for categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and store as pandas dataframe\n",
    "filename = '../Data/day.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# dropped_attributes = ['instant','dteday','yr','mnth','holiday','weekday','casual','registered']\n",
    "dropped_attributes = ['instant','dteday','yr','mnth','holiday','weekday']\n",
    "for attribute in dropped_attributes:\n",
    "    df = df.drop(attribute, axis=1)\n",
    "    \n",
    "# One hot encoding\n",
    "ohe_df = pd.get_dummies(df, columns = ['season','weathersit'])\n",
    "ohe_df = ohe_df.drop(['season_4','weathersit_3'], axis=1)    # season_4, weathersit_3 are chosen as reference variables\n",
    "display(ohe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d36af",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = ohe_df.shape\n",
    "attribute_names = list(ohe_df.columns)\n",
    "\n",
    "# Get column indexes\n",
    "temp_col = ohe_df.columns.get_loc(\"temp\")\n",
    "atemp_col = ohe_df.columns.get_loc(\"atemp\")\n",
    "hum_col = ohe_df.columns.get_loc(\"hum\")\n",
    "wspd_col = ohe_df.columns.get_loc(\"windspeed\")\n",
    "cnt_col = ohe_df.columns.get_loc(\"cnt\")\n",
    "\n",
    "# Undo the original max-min normalization\n",
    "data = ohe_df.values\n",
    "for row in range(0, N):\n",
    "    data[row, temp_col] = data[row, temp_col]*(39-(-8)) + (-8)\n",
    "    data[row, atemp_col] = data[row, atemp_col]*(50-(-16)) + (-16)\n",
    "    data[row, hum_col] = data[row, hum_col]*100\n",
    "    data[row, wspd_col] = data[row, wspd_col]*67\n",
    "\n",
    "# Standarize ratio data attributes\n",
    "for col in range(temp_col, cnt_col+1): # subtract mean column-wise\n",
    "    mn = data[:, col].mean(0)\n",
    "    std = np.std(data[:, col])\n",
    "    data[:, col] = (data[:, col] - np.ones(N)*mn)/std\n",
    "\n",
    "# Create DataFrame for visualisation\n",
    "data_df = pd.DataFrame(data, columns=attribute_names)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388b967",
   "metadata": {},
   "source": [
    "### Set 'workingday' as target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066153f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into features and target vector\n",
    "workingday = attribute_names.index(\"workingday\")\n",
    "y = data[:,workingday]\n",
    "X = np.delete(data, workingday, axis=1)\n",
    "attribute_names.pop(workingday)\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "X = np.concatenate((np.ones((X.shape[0],1)),X),1)\n",
    "attribute_names = [u'offset']+attribute_names\n",
    "M = M+1\n",
    "\n",
    "print(\"N: {}, M: {} (including offset)\".format(N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75170046",
   "metadata": {},
   "source": [
    "### 1. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of lambda\n",
    "lambdas = np.power(10.,np.linspace(-5,9,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedb64e",
   "metadata": {},
   "source": [
    "### 2. K-Nearest-Neighbours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75574291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neighbours\n",
    "k_start = 1\n",
    "k_end = 100\n",
    "k_values = list(range(k_start,k_end+1))\n",
    "dist = 2\n",
    "metric = 'minkowski'\n",
    "metric_params = {}  # no parameters needed for minkowski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89e582",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba99068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "K_outer = 10\n",
    "K_inner = 10\n",
    "\n",
    "cv_outer = model_selection.KFold(K_outer, shuffle=True, random_state=7)\n",
    "cv_inner = model_selection.KFold(K_inner, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "Error_test_rlr = np.empty((K_outer,1))\n",
    "Error_test_base = np.empty((K_outer,1))\n",
    "opt_lambdas = np.empty((K_outer,1))             # optimal lambdas for each outer fold\n",
    "\n",
    "Error_test_KNN = np.empty((K_outer,1))\n",
    "opt_k_values = np.empty((K_outer,1))             # optimal neighbours for each outer fold\n",
    "\n",
    "mu = np.empty((K_outer, M-1))\n",
    "sigma = np.empty((K_outer, M-1))\n",
    "\n",
    "# Statistical evaluation\n",
    "y_hat = np.zeros((1,3))       # estimates for each model [KNN, rlr, baseline], to delete 1st row at the end\n",
    "y_true = np.zeros((1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a66ba",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103a176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k_out, (par_index, test_index) in enumerate(cv_outer.split(X,y)):\n",
    "    \n",
    "    print('\\nOuter Cross Validation Fold: {0}/{1}'.format(k_out+1,K_outer))\n",
    "    \n",
    "    # Split outer fold into parameterisation set and test set\n",
    "    X_par = X[par_index]\n",
    "    y_par = y[par_index].astype(int)\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index].astype(int)\n",
    "    y_par = y_par.squeeze()\n",
    "    \n",
    "    # Initialise error arrays\n",
    "    w = np.empty((M,K_inner,len(lambdas)))           \n",
    "    lr_train_error = np.empty((K_inner,len(lambdas)))\n",
    "    lr_val_error = np.empty((K_inner,len(lambdas)))    \n",
    "    coefficient_norm = np.empty((K_inner,len(lambdas)))   \n",
    "    \n",
    "    knn_train_error = np.empty((K_inner,len(k_values)))\n",
    "    knn_val_error = np.empty((K_inner,len(k_values)))    \n",
    "    \n",
    "    for k_in, (train_index, val_index) in enumerate(cv_inner.split(X_par,y_par)):\n",
    "    \n",
    "        print('\\n\\tInner Fold: {}/{}'.format(k_in+1,K_inner))\n",
    "        # Split parameterisation set into training set and validation set\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index].astype(int)\n",
    "        X_val = X[val_index]\n",
    "        y_val = y[val_index].astype(int)\n",
    "        \n",
    "        # Standardize inner fold based on training set, and save the mean and std\n",
    "        mu_train = np.mean(X_train[:, 1:], 0)\n",
    "        sigma_train = np.std(X_train[:, 1:], 0)\n",
    "        X_train[:, 1:] = (X_train[:, 1:] - mu_train) / sigma_train\n",
    "        X_val[:, 1:] = (X_val[:, 1:] - mu_train) / sigma_train\n",
    "    \n",
    "        ########################### Logistic Regression ###########################\n",
    "        # solve for weights\n",
    "        for l in range(0,len(lambdas)):\n",
    "            mdl = LogisticRegression(penalty='l2', C=1/lambdas[l] )\n",
    "            mdl.fit(X_train, y_train)\n",
    "            w[:,k_in,l] = mdl.coef_[0]\n",
    "            # w[:,k_in,l] = np.sqrt(np.sum(w_est**2))\n",
    "            \n",
    "            y_train_est = mdl.predict(X_train).T.astype(int)\n",
    "            y_val_est = mdl.predict(X_val).T.astype(int)\n",
    "                    \n",
    "            # Evaluate training and validation performance\n",
    "            lr_train_error[k_in,l] = np.sum(y_train_est != y_train) / len(y_train)\n",
    "            lr_val_error[k_in,l] = np.sum(y_val_est != y_val) / len(y_val)\n",
    "        \n",
    "        ################################ KNN #################################\n",
    "        for k_i in range(0,len(k_values)):\n",
    "            knclassifier = KNeighborsClassifier(n_neighbors=k_values[k_i], p=dist,\n",
    "                                    metric=metric,\n",
    "                                    metric_params=metric_params)\n",
    "            knclassifier.fit(X_train, y_train)\n",
    "            \n",
    "            y_train_est = knclassifier.predict(X_train)\n",
    "            y_val_est = knclassifier.predict(X_val)\n",
    "            \n",
    "            # Evaluate training and validation performance\n",
    "            knn_train_error[k_in,k_i] = np.sum(y_train_est != y_train) / len(y_train)\n",
    "            knn_val_error[k_in,k_i] = np.sum(y_val_est != y_val) / len(y_val)\n",
    "\n",
    "    ########################### Logistic Regression ###########################\n",
    "    opt_val_err = np.min(lr_val_error)\n",
    "    opt_lambda = lambdas[np.argmin(np.mean(lr_val_error,axis=0))]\n",
    "    opt_lambdas[k_out] = opt_lambda\n",
    "    print(\"Optimal lambda found: \", round(np.log10(opt_lambda),5))\n",
    "   \n",
    "    train_err_vs_lambda = np.mean(lr_train_error,axis=0)\n",
    "    val_err_vs_lambda = np.mean(lr_val_error,axis=0)\n",
    "    mean_w_vs_lambda = np.squeeze(np.mean(w,axis=1))\n",
    "\n",
    "    # Retrain logistic regression model on parameterisation set using optimal lambda\n",
    "    mdl = LogisticRegression(penalty='l2', C=1/opt_lambda )\n",
    "    mdl.fit(X_par, y_par)\n",
    "\n",
    "    # y_par_est = mdl.predict(X_par).T.astype(int)\n",
    "    y_test_est = mdl.predict(X_test).T.astype(int)\n",
    "\n",
    "    # Evaluate test performance\n",
    "    Error_test_rlr[k_out] = np.sum(y_test_est != y_test) / len(y_test)\n",
    "    \n",
    "    ########################### Baseline Model ###########################\n",
    "    majority_class = np.bincount(y_test).argmax()\n",
    "    Error_test_base[k_out] = np.sum(y_test != majority_class) / len(y_test)\n",
    "    \n",
    "    # For statistical evaluation\n",
    "    y_hat_rlr = np.expand_dims(y_test_est, axis=1)\n",
    "    y_hat_base = np.full((y_test.shape[0], 1), majority_class)\n",
    "    y_true = np.append(y_true, y_test, axis=0)\n",
    "    \n",
    "    ################################ KNN #################################\n",
    "    opt_k_err = np.min(knn_val_error)\n",
    "    opt_k = k_values[np.argmin(np.mean(knn_val_error,axis=0))]\n",
    "    opt_k_values[k_out] = opt_k\n",
    "    print(\"Optimal k found: \", opt_k)\n",
    "    \n",
    "    # Retrain KNN model on parameterisation set using optimal lambda\n",
    "    knclassifier = KNeighborsClassifier(n_neighbors=opt_k, p=dist,\n",
    "                                    metric=metric,\n",
    "                                    metric_params=metric_params)\n",
    "    knclassifier.fit(X_par, y_par)\n",
    "    y_test_est = knclassifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate test performance\n",
    "    Error_test_KNN[k_out] = np.sum(y_test_est != y_test) / len(y_test)\n",
    "    \n",
    "    # For statistical evaluation\n",
    "    y_hat_knn = np.expand_dims(y_test_est, axis=1)\n",
    "    y_hat_fold = np.concatenate((y_hat_knn, y_hat_rlr, y_hat_base), axis=1)\n",
    "    y_hat = np.append(y_hat, y_hat_fold, axis=0)\n",
    "    \n",
    "   # Display the results for the last outer cross-validation fold\n",
    "    if k_out == K_outer-1:\n",
    "        reg_fig, reg_axes = plt.subplots(1, 2, figsize=(10,5),num=1)\n",
    "        reg_axes[0].semilogx(lambdas,mean_w_vs_lambda.T[:,1:],'-',label=attribute_names[1:]) # Don't plot the bias term\n",
    "        reg_axes[0].set_xlabel('Regularization factor')\n",
    "        reg_axes[0].set_ylabel('Mean Coefficient Values')\n",
    "        reg_axes[0].grid()\n",
    "        reg_axes[0].legend()\n",
    "\n",
    "        reg_axes[1].set_title('Optimal lambda: 1e{0}'.format(round(np.log10(opt_lambdas[k_out][0]),5)))\n",
    "        reg_axes[1].loglog(lambdas,train_err_vs_lambda.T,'b-',lambdas,val_err_vs_lambda.T,'r-')\n",
    "        reg_axes[1].set_xlabel('Regularization factor')\n",
    "        reg_axes[1].set_ylabel('Squared error (crossvalidation)')\n",
    "        reg_axes[1].legend(['Train error','Validation error'])\n",
    "        reg_axes[1].grid()\n",
    "        reg_fig.tight_layout()        \n",
    "        \n",
    "# For statistical evaluation\n",
    "y_hat = np.delete(y_hat, 0, 0)      # delete the 1st row of zeros\n",
    "y_true = np.delete(y_true, 0, 0)      # delete the 1st row of zeros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103de95",
   "metadata": {},
   "source": [
    "### Plot Weights from Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bar plot of RLR weights\n",
    "# weights = range(1,M)      # skip offset\n",
    "# bw = 1.0/(len(weights)+1)\n",
    "# r = np.arange(1,K_outer+1)\n",
    "\n",
    "# plt.figure(figsize=(6,6), num=3)\n",
    "# for i in weights:\n",
    "#     plt.bar(r+i*bw, w[i,:], width=bw)\n",
    "# plt.xticks(r+bw, range(1,K_outer+1))\n",
    "# plt.xlabel('Attributes')\n",
    "# plt.ylabel('Weights')\n",
    "# plt.legend(attribute_names[1:M+1], loc=(1.04, 0))\n",
    "# plt.grid()\n",
    "# plt.title('Weights from each fold of Regularized Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7016d51",
   "metadata": {},
   "source": [
    "### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c20db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Outer fold \\tKNN \\t\\t\\tLogistic Regression \\t\\tBaseline\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"i \\t\\tk_i \\tEtest_i \\tlambda_i \\tEtest_i \\tEtest_i\")\n",
    "for i in range(K_outer):\n",
    "    print(\"{} \\t\\t{:.3f} \\t{:.3f} \\t\\t1e{} \\t{:.3f} \\t\\t{:.3f}\".format(\\\n",
    "        i+1,opt_k_values[i][0],Error_test_KNN[i].item(),round(np.log10(opt_lambdas[i][0]),3),\\\n",
    "            Error_test_rlr[i][0],Error_test_base[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292093d",
   "metadata": {},
   "source": [
    "## Statistical Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e83d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mcnemar(thetahat, CI, p):\n",
    "    print(\"\\t CI : ({:.5f}, {:.5f})\".format(CI[0], CI[1]))\n",
    "    print(\"\\t p: {:.5e}\".format(p))\n",
    "    print(\"\\t theta_hat: {:.5f}\\n\".format(thetahat))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deaece2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [KNN, RLR, Baseline]\n",
    "alpha = 0.05\n",
    "[thetahat_1, CI_1, p_1] = mcnemar(y_true, y_hat[:,0], y_hat[:,1], alpha=alpha)    # KNN vs RLR\n",
    "print(\"\\n\")\n",
    "[thetahat_2, CI_2, p_2] = mcnemar(y_true, y_hat[:,0], y_hat[:,2], alpha=alpha)    # KNN vs Baseline\n",
    "print(\"\\n\")\n",
    "[thetahat_3, CI_3, p_3] = mcnemar(y_true, y_hat[:,1], y_hat[:,2], alpha=alpha)    # RLR vs Baseline\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print_mcnemar(thetahat_1, CI_1, p_1)    # RLR better than KNN\n",
    "print_mcnemar(thetahat_2, CI_2, p_2)    # KNN better than baseline\n",
    "print_mcnemar(thetahat_3, CI_3, p_3)    # RLR better than baseline\n",
    "\n",
    "# positive theta_hat means A better than B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN Confusion Matrix\")\n",
    "print(metrics.confusion_matrix(y_true, y_hat[:,0]))\n",
    "\n",
    "print(\"\\nRegularised Logistic Regression Confusion Matrix\")\n",
    "print(metrics.confusion_matrix(y_true, y_hat[:,1]))\n",
    "\n",
    "print(\"\\nBaseline Confusion Matrix\")\n",
    "print(metrics.confusion_matrix(y_true, y_hat[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c078667",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [np.sum(y_hat[:,i]==y_true)/len(y_true) for i in range(y_hat.shape[1])]\n",
    "accuracy_raw = [np.sum(y_hat[:,i]==y_true) for i in range(y_hat.shape[1])]\n",
    "\n",
    "print(\"accuracy %: \", accuracy)\n",
    "print(\"accuracy raw: \", accuracy_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5371e47",
   "metadata": {},
   "source": [
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "# dump_session('sun_night_2_env.db')\n",
    "# dill.load_session('friday_2234_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d69a96",
   "metadata": {},
   "source": [
    "## Saved Results Below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a298d517",
   "metadata": {},
   "source": [
    "Outer fold \tKNN \t\t\tLogistic Regression \t\tBaseline\n",
    "--------------------------------------------------------------------------------\n",
    "i \t\tk_i \tEtest_i \tlambda_i \tEtest_i \tEtest_i\n",
    "1 \t\t4.000 \t0.095 \t\t1e-5.0 \t0.081 \t\t0.378\n",
    "2 \t\t5.000 \t0.082 \t\t1e-0.333 \t0.027 \t\t0.288\n",
    "3 \t\t5.000 \t0.151 \t\t1e-0.333 \t0.041 \t\t0.301\n",
    "4 \t\t5.000 \t0.178 \t\t1e-0.333 \t0.055 \t\t0.219\n",
    "5 \t\t5.000 \t0.123 \t\t1e-0.333 \t0.068 \t\t0.315\n",
    "6 \t\t5.000 \t0.137 \t\t1e-0.333 \t0.055 \t\t0.260\n",
    "7 \t\t5.000 \t0.178 \t\t1e-0.333 \t0.068 \t\t0.315\n",
    "8 \t\t5.000 \t0.137 \t\t1e-0.333 \t0.151 \t\t0.356\n",
    "9 \t\t5.000 \t0.096 \t\t1e-0.333 \t0.082 \t\t0.438\n",
    "10 \t\t5.000 \t0.123 \t\t1e-0.333 \t0.068 \t\t0.288"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course02450",
   "language": "python",
   "name": "course02450"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
